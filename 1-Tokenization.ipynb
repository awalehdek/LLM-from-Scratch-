{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa873a4-2d25-48d2-8096-0efc93eaee16",
   "metadata": {},
   "source": [
    "# Create Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99648962-cc52-4239-9975-1d19a7ba474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\awalehdek\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84f87e66-f7f2-4731-a93b-8622311b8abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 21918\n",
      "1The Verdict\n",
      "Edith Wharton\n",
      "1908\n",
      "Exported from Wikisource on May 20, 20242I HAD always thought Jack Gisburn rather a cheap genius--\n",
      "though a good fellow enough--so it was no great surprise to\n",
      "me to hear that, in the height of his glory , he had dropped\n",
      "his painting, married a rich widow , and established himself\n",
      "in a villa on the Riviera. (Though I rather thought it would\n",
      "have been Rome or Florence.)\n",
      "\"The height of his glory\"--that was what the women called\n",
      "it. I can hear Mrs. Gideon Thwing--his last Chicago sitter --\n",
      "deploring his unaccountable abdication. \"Of course it's\n",
      "going to send the value of my picture 'way up; but I don't\n",
      "think of that, Mr . Rickham--the loss to Arrt is all I think of.\"\n",
      "The word, on Mrs. Thwing's lips, multiplied its _rs_ as\n",
      "though they were reflected in an endless vista of mirrors.\n",
      "And it was not only the Mrs. Thwings who mourned. Had\n",
      "not the exquisite  Hermia Croft, at the last Grafton Gallery\n",
      "show , stopped me before Gisburn's \"Moon-dancers\" to say,\n",
      "with te\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "# Open in binary mode\n",
    "with open(\"The_Verdict.pdf\", \"rb\") as fp:\n",
    "    reader = PyPDF2.PdfReader(fp)\n",
    "    text_book = \"\"\n",
    "    for page in reader.pages:\n",
    "        text_book += page.extract_text() or \"\"  # Avoid None\n",
    "\n",
    "print(\"Total number of characters:\", len(text_book))\n",
    "print(text_book[:999])\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4559df-768b-4ff0-9c71-14a94796a276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today', 'is', 'sunny,', \"let's\", '--', 'go', 'to', 'the', 'beach!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "text1  = \"Today is sunny, let's -- go to the beach!\"\n",
    "# split text around empty space\n",
    "res = re.split(r'\\s', text1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72fec4c2-97f7-4b6c-9d4e-247fb15ba94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today', 'is', 'sunny', '', \"let's\", '--', 'go', 'to', 'the', 'beach!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split text around empty space, comman and period\n",
    "res = re.split(r'[,.]|\\s', text1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "424b31bd-cbfb-4e44-bff2-102a905ad07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: ['Today', 'is', 'sunny', ',', \"let's\", '--', 'go', 'to', 'the', 'beach', '!']\n",
      "res1: ['Today', 'is', 'sunny', ',', \"let's\", '--', 'go', 'to', 'the', 'beach', '!']\n"
     ]
    }
   ],
   "source": [
    "res = re.split(r'([,.:?!\",()]|--|\\s)', text1)\n",
    "\n",
    "# Removing white spaces\n",
    "res = [word for word in res if word.strip() ]\n",
    "print(\"res:\", res)\n",
    "\n",
    "# Alternative: Keep words AND punctuation as separate tokens but no empty strings\n",
    "res1 =  re.findall(r'[,.:?!\",()]|--|[^,.:?!\",()\\s]+', text1) \n",
    "print(\"res1:\", res1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9844461-650e-4080-8ffe-01b16e0b2290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: ['Today', 'is', 'sunny', ',', \"let's\", '--', 'go', 'to', 'the', 'beach', '!']\n",
      "res1: ['Today', 'is', 'sunny', \"let's\", 'go', 'to', 'the', 'beach']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awalehdek\\AppData\\Local\\Temp\\ipykernel_8720\\444435906.py:6: FutureWarning: Possible set difference at position 13\n",
      "  res1 = re.findall(r'[^\\s,.:?!\",()--]+|(?<=\\w)\\'(?=\\w)', text1)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize by removing all space, comma, --, etcc\n",
    "res = [word.strip() for word in res if word.strip() ]\n",
    "print(\"res:\",res)\n",
    "\n",
    "# If you want to keep apostrophes in words like \"let's\":\n",
    "res1 = re.findall(r'[^\\s,.:?!\",()--]+|(?<=\\w)\\'(?=\\w)', text1)\n",
    "print(\"res1:\", res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cfff15f-b80e-4d67-8ffd-54e6436d55a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess:\n",
      " ['1The', 'Verdict', 'Edith', 'Wharton', '1908', 'Exported', 'from', 'Wikisource', 'on', 'May', '20,', '20242I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap']\n",
      "preprocess_1:\n",
      " ['1The', 'Verdict', 'Edith', 'Wharton', '1908', 'Exported', 'from', 'Wikisource', 'on', 'May', '20,', '20242I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awalehdek\\AppData\\Local\\Temp\\ipykernel_8720\\823624902.py:10: FutureWarning: Possible set difference at position 13\n",
      "  preprocess_1 = re.findall(r'[^\\s,.:?!\",()--]+|[,.:?!\",()]|--', text_book)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing Case 1:\n",
    "# Split text on punctuation, symbols, or whitespace, keeping delimiters\n",
    "preprocess = re.split(r'( [,.:?!\",()]|--|\\s)', text_book)\n",
    "# Remove empty strings and whitespace-only elements\n",
    "preprocess = [word.strip() for word in preprocess if word.strip() ]\n",
    "print('preprocess:\\n',preprocess[:20])\n",
    "\n",
    "# preprocessing Case 2:\n",
    "# Alternative approaches Extract words and punctuation as separate tokens\n",
    "preprocess_1 = re.findall(r'[^\\s,.:?!\",()--]+|[,.:?!\",()]|--', text_book)\n",
    "# Extract all non-whitespace chunks (includes punctuation attached to words)\n",
    "preprocess_1 = re.findall(r'[^\\s]+', text_book)\n",
    "print('preprocess_1:\\n',preprocess_1[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb542d0-2630-421d-94ac-f54da2e29812",
   "metadata": {},
   "source": [
    "# get the unique word vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "365f6fea-efe6-4164-be18-12a84afaf4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vocabulary:_________\n",
      "0: !\n",
      "1: \"\n",
      "2: \"Ah\n",
      "3: \"Ah,\n",
      "4: \"Begin\n",
      "5: \"By\n",
      "6: \"Destroyed\n",
      "7: \"Don't\n",
      "8: \"Hang\n",
      "9: \"Has\n",
      "\n",
      "string_to_int:_____________\n",
      "!: 0\n",
      "\": 1\n",
      "\"Ah: 2\n",
      "\"Ah,: 3\n",
      "\"Begin: 4\n",
      "\"By: 5\n",
      "\"Destroyed: 6\n",
      "\"Don't: 7\n",
      "\"Hang: 8\n",
      "\"Has: 9\n"
     ]
    }
   ],
   "source": [
    "# option  1\n",
    "from itertools import islice\n",
    "w = sorted(set(preprocess))\n",
    "vocabulary = {key: value for key, value in enumerate(w)}\n",
    "# Print top 10 items\n",
    "print(\"\\nvocabulary:_________\")\n",
    "for key, value in islice(vocabulary.items(), 10):\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "w = sorted(set(preprocess))\n",
    "vocabulary = {key: value for key, value in enumerate(w)}\n",
    "# Fix: This should be {word: idx} not {idx: word}\n",
    "string_to_int = {word: idx for idx, word in enumerate(w)}\n",
    "print(\"\\nstring_to_int:_____________\")\n",
    "for key, value in islice(string_to_int.items(), 10):\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb157a-2620-49e0-aa1e-239c9a49aba5",
   "metadata": {},
   "source": [
    "# Encode and Decode \n",
    "## Endcode: Simple Text -> Token text -> Token ID\n",
    "1. Simple Text (Raw Input):   \"Hello, world!\"\n",
    "2. Tokenized Text (Splitting into Tokens):\n",
    "\n",
    "               A- Whole words (\"Hello\", \"world\")\n",
    "               B- Subwords (\"unhappiness\" â†’ \"un\", \"happiness\")\n",
    "               C- Punctuation and symbols (\",\", \"!\")\n",
    "               Tokenized Output: [\"Hello\", \",\", \"world\", \"!\"]\n",
    "3. Token IDs (Numerical Representation):\n",
    "\n",
    "                \"Hello\" â†’ 15496\n",
    "                \",\" â†’ 11\n",
    "                \"world\" â†’ 995\n",
    "                \"!\" â†’ 0\n",
    "                Final Token IDs: [15496, 11, 995, 0]\n",
    "## Decode: Token ID -> Token Text -> Simple Text\n",
    "    \n",
    "    Decode is the reverse: Token id -> Token Text -> Simple Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdb4406e-8c9c-4a35-9371-b76a749caa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1\n",
    "class Tokenization:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.string_to_int = vocabulary\n",
    "        self.int_to_string = {item: s for s,item in vocabulary.items()}\n",
    "\n",
    "    def encoding(self, text):\n",
    "        # split text around empty space, comman and period\n",
    "        preprocessing = re.split(r'([,.:?!\",()]|--|\\s)', text)\n",
    "        # Tokenize by removing all space, comma, --, etcc\n",
    "        preprocessing = [word.strip() for word in preprocessing if word.strip() ]\n",
    "        # converting into id number\n",
    "        id = [self.string_to_int[s] for s in preprocessing ]\n",
    "        return id \n",
    "\n",
    "    def decoding(self, vocabulary ):\n",
    "        # convert token id to Token text\n",
    "        tokenID_to_TokenText = \" \".join([self.int_to_string[i] for i in id])\n",
    "        # replace empty space with punctuation\n",
    "        tokenID_to_TokenText =  re.sub (r'([,.:?!\",()]|--|\\s)', r'\\1', tokenID_to_TokenText)\n",
    "        return tokenID_to_TokenText\n",
    "# option 2\n",
    "class Tokenization_1:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.string_to_int = vocabulary  # Should be {word: id}\n",
    "        self.int_to_string = {id: word for word, id in vocabulary.items()}  # Reverse mapping\n",
    "\n",
    "    def encoding_1(self, text):\n",
    "        # Extract tokens directly (no empty strings)\n",
    "        tokens = re.findall(r'[^\\s,.:?!\",()--]+|[,.:?!\",()]|--', text)\n",
    "        # Convert to IDs using dictionary lookup\n",
    "        return [self.string_to_int[token] for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02488619-bf3a-4606-8f19-67f56f660845",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'height'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m Tok = Tokenization(vocabulary)\n\u001b[32m      3\u001b[39m inputText = \u001b[33m\"\u001b[39m\u001b[33mheight of his glory that was what the women called it.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tokenID = \u001b[43mTok\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputText\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenID)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mTokenization.encoding\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     11\u001b[39m preprocessing = [word.strip() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m preprocessing \u001b[38;5;28;01mif\u001b[39;00m word.strip() ]\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# converting into id number\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28mid\u001b[39m = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstring_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessing ]\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mid\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'height'"
     ]
    }
   ],
   "source": [
    "# unseen word give us error because of short data\n",
    "Tok = Tokenization(vocabulary)\n",
    "inputText = \"height of his glory that was what the women called it.\"\n",
    "tokenID = Tok.encoding(inputText)\n",
    "print(tokenID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376c2bab-9505-4b61-b102-03a17824f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to deal with unseen token\n",
    "# Assume preprocessing is your list of tokens from dataset\n",
    "unseen_token = sorted(list(set(preprocessing)))\n",
    "\n",
    "# Add special tokens explicitly\n",
    "unseen_token.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "# Assign each token an integer ID\n",
    "vocab = {tok: num for num, tok in enumerate(unseen_token)}\n",
    "\n",
    "# Vocabulary size\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942e1b4-b666-4e7f-bb2c-2a59d3a4a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When your model encounters a word not in vocab, you map it to <|unk|>:\n",
    "def get_token_id(token, vocab):\n",
    "    return vocab.get(token, vocab[\"<|unk|>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef41a3e3-6f7d-4a4b-9365-d923c63349f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.11.0-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2025.9.1-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\awalehdek\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\awalehdek\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\awalehdek\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\awalehdek\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\awalehdek\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
      "Downloading tiktoken-0.11.0-cp313-cp313-win_amd64.whl (883 kB)\n",
      "   ---------------------------------------- 0.0/883.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/883.9 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 524.3/883.9 kB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 786.4/883.9 kB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 883.9/883.9 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading regex-2025.9.1-cp313-cp313-win_amd64.whl (275 kB)\n",
      "Installing collected packages: regex, tiktoken\n",
      "\n",
      "   ---------------------------------------- 0/2 [regex]\n",
      "   ---------------------------------------- 0/2 [regex]\n",
      "   ---------------------------------------- 0/2 [regex]\n",
      "   ---------------------------------------- 0/2 [regex]\n",
      "   ---------------------------------------- 0/2 [regex]\n",
      "   -------------------- ------------------- 1/2 [tiktoken]\n",
      "   -------------------- ------------------- 1/2 [tiktoken]\n",
      "   -------------------- ------------------- 1/2 [tiktoken]\n",
      "   -------------------- ------------------- 1/2 [tiktoken]\n",
      "   ---------------------------------------- 2/2 [tiktoken]\n",
      "\n",
      "Successfully installed regex-2025.9.1 tiktoken-0.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0becd7e4-0095-4f49-959d-e628d0f81809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f3eee3-4daa-4ca3-8fcc-52fead0a512e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m text = (\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHello, would like coffee? <|endoftext|> In the  morning in your room\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m      \u001b[33m\"\u001b[39m\u001b[33mof windburry.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m integers = \u001b[43mtokenizer\u001b[49m.encode(text, allowed_special={\u001b[33m\"\u001b[39m\u001b[33m<|endoftext|>\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(integers)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, would like coffee? <|endoftext|> In the  morning in your room\"\n",
    "     \"of windburry.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500e579-f0a8-40a8-9b69-d352c164509c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
