{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa873a4-2d25-48d2-8096-0efc93eaee16",
   "metadata": {},
   "source": [
    "# Create Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99648962-cc52-4239-9975-1d19a7ba474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\awalehdek\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f87e66-f7f2-4731-a93b-8622311b8abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 21918\n",
      "1The Verdict\n",
      "Edith Wharton\n",
      "1908\n",
      "Exported from Wikisource on May 20, 20242I HAD always thought Jack Gisburn rather a cheap genius--\n",
      "though a good fellow enough--so it was no great surprise to\n",
      "me to hear that, in the height of his glory , he had dropped\n",
      "his painting, married a rich widow , and established himself\n",
      "in a villa on the Riviera. (Though I rather thought it would\n",
      "have been Rome or Florence.)\n",
      "\"The height of his glory\"--that was what the women called\n",
      "it. I can hear Mrs. Gideon Thwing--his last Chicago sitter --\n",
      "deploring his unaccountable abdication. \"Of course it's\n",
      "going to send the value of my picture 'way up; but I don't\n",
      "think of that, Mr . Rickham--the loss to Arrt is all I think of.\"\n",
      "The word, on Mrs. Thwing's lips, multiplied its _rs_ as\n",
      "though they were reflected in an endless vista of mirrors.\n",
      "And it was not only the Mrs. Thwings who mourned. Had\n",
      "not the exquisite  Hermia Croft, at the last Grafton Gallery\n",
      "show , stopped me before Gisburn's \"Moon-dancers\" to say,\n",
      "with te\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "# Open in binary mode\n",
    "with open(\"The_Verdict.pdf\", \"rb\") as fp:\n",
    "    reader = PyPDF2.PdfReader(fp)\n",
    "    text_book = \"\"\n",
    "    for page in reader.pages:\n",
    "        text_book += page.extract_text() or \"\"  # Avoid None\n",
    "\n",
    "print(\"Total number of characters:\", len(text_book))\n",
    "print(text_book[:999])\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c4559df-768b-4ff0-9c71-14a94796a276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today', 'is', 'sunny,', \"let's\", '--', 'go', 'to', 'the', 'beach!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "text1  = \"Today is sunny, let's -- go to the beach!\"\n",
    "# split text around empty space\n",
    "res = re.split(r'\\s', text1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72fec4c2-97f7-4b6c-9d4e-247fb15ba94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today', 'is', 'sunny', '', \"let's\", '--', 'go', 'to', 'the', 'beach!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split text around empty space, comman and period\n",
    "res = re.split(r'[,.]|\\s', text1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "424b31bd-cbfb-4e44-bff2-102a905ad07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today', 'is', 'sunny', ',', \"let's\", '--', 'go', 'to', 'the', 'beach', '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = re.split(r'([,.:?!\",()]|--|\\s)', text1)\n",
    "\n",
    "# Removing white spaces\n",
    "res = [word for word in res if word.strip() ]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9844461-650e-4080-8ffe-01b16e0b2290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today', 'is', 'sunny', ',', \"let's\", '--', 'go', 'to', 'the', 'beach', '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize by removing all space, comma, --, etcc\n",
    "res = [word.strip() for word in res if word.strip() ]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cfff15f-b80e-4d67-8ffd-54e6436d55a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1The', 'Verdict', 'Edith', 'Wharton', '1908', 'Exported', 'from', 'Wikisource', 'on', 'May', '20,', '20242I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap']\n"
     ]
    }
   ],
   "source": [
    "# preprocessing \n",
    "preprocess = re.split(r'( [,.:?!\",()]|--|\\s)', text_book)\n",
    "preprocess = [word.strip() for word in preprocess if word.strip() ]\n",
    "print(preprocess[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb542d0-2630-421d-94ac-f54da2e29812",
   "metadata": {},
   "source": [
    "# get the unique word vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "365f6fea-efe6-4164-be18-12a84afaf4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '!',\n",
       " 1: '\"',\n",
       " 2: '\"Ah',\n",
       " 3: '\"Ah,',\n",
       " 4: '\"Begin',\n",
       " 5: '\"By',\n",
       " 6: '\"Destroyed',\n",
       " 7: '\"Don\\'t',\n",
       " 8: '\"Hang',\n",
       " 9: '\"Has',\n",
       " 10: '\"I',\n",
       " 11: '\"I\\'d',\n",
       " 12: '\"If',\n",
       " 13: '\"It',\n",
       " 14: '\"It\\'s',\n",
       " 15: '\"Jack',\n",
       " 16: '\"Mr.',\n",
       " 17: '\"My',\n",
       " 18: '\"Never',\n",
       " 19: '\"Oh,',\n",
       " 20: '\"Once,',\n",
       " 21: '\"Only',\n",
       " 22: '\"Or',\n",
       " 23: '\"That',\n",
       " 24: '\"The',\n",
       " 25: '\"Then',\n",
       " 26: '\"This',\n",
       " 27: '\"Well,',\n",
       " 28: '\"What',\n",
       " 29: '\"When',\n",
       " 30: '\"Why',\n",
       " 31: '\"Yes',\n",
       " 32: '\"You',\n",
       " 33: '\"but',\n",
       " 34: \"'Are\",\n",
       " 35: \"'It's\",\n",
       " 36: \"'coming'\",\n",
       " 37: \"'done'\",\n",
       " 38: \"'subject.'\",\n",
       " 39: \"'technique'\",\n",
       " 40: \"'way\",\n",
       " 41: '(',\n",
       " 42: ',',\n",
       " 43: '--',\n",
       " 44: '-colour',\n",
       " 45: '-presses',\n",
       " 46: '.',\n",
       " 47: '1,',\n",
       " 48: '1908',\n",
       " 49: '1929.',\n",
       " 50: '1The',\n",
       " 51: '1above',\n",
       " 52: '20,',\n",
       " 53: '2021About',\n",
       " 54: '20242I',\n",
       " 55: '4.0',\n",
       " 56: 'A',\n",
       " 57: 'Abigor',\n",
       " 58: 'AdamBMor',\n",
       " 59: 'Among',\n",
       " 60: 'And',\n",
       " 61: 'And,',\n",
       " 62: 'Arrt',\n",
       " 63: 'As',\n",
       " 64: 'At',\n",
       " 65: 'Attribution-ShareAlike',\n",
       " 66: 'AzaT',\n",
       " 67: 'Be',\n",
       " 68: 'Bender235',\n",
       " 69: 'Blurpeace',\n",
       " 70: 'Boris23',\n",
       " 71: 'Bromskloss',\n",
       " 72: 'Burlington',\n",
       " 73: 'But',\n",
       " 74: 'But,',\n",
       " 75: 'By',\n",
       " 76: 'Carlo;',\n",
       " 77: 'Chicago',\n",
       " 78: 'Claude',\n",
       " 79: 'Come',\n",
       " 80: 'Commons',\n",
       " 81: 'Creative',\n",
       " 82: 'Croft',\n",
       " 83: 'Croft)',\n",
       " 84: 'Croft,',\n",
       " 85: 'Dbenbenn22Zscout370',\n",
       " 86: 'Devonshire',\n",
       " 87: 'Dha',\n",
       " 88: \"Don't\",\n",
       " 89: 'Dschwen',\n",
       " 90: 'Dubarry_',\n",
       " 91: 'During',\n",
       " 92: 'Edith',\n",
       " 93: 'Emperors',\n",
       " 94: 'Exported',\n",
       " 95: 'FDL',\n",
       " 96: 'Florence.)',\n",
       " 97: 'For',\n",
       " 98: 'GNU',\n",
       " 99: 'Gallery',\n",
       " 100: 'Gideon',\n",
       " 101: 'Gisburn',\n",
       " 102: 'Gisburn!',\n",
       " 103: \"Gisburn's\",\n",
       " 104: 'Gisburn,',\n",
       " 105: 'Gisburns\"',\n",
       " 106: 'Grafton',\n",
       " 107: 'Greek',\n",
       " 108: 'Grindle',\n",
       " 109: \"Grindle's\",\n",
       " 110: 'Grindle,\"',\n",
       " 111: 'Grindle.',\n",
       " 112: 'Grindle:',\n",
       " 113: 'Grindles.\"',\n",
       " 114: 'HAD',\n",
       " 115: 'Had',\n",
       " 116: 'He',\n",
       " 117: 'Her12only',\n",
       " 118: 'Hermia',\n",
       " 119: \"Hermia's\",\n",
       " 120: 'His',\n",
       " 121: 'I',\n",
       " 122: \"I'd\",\n",
       " 123: \"I'll\",\n",
       " 124: \"I've\",\n",
       " 125: 'If',\n",
       " 126: 'In',\n",
       " 127: 'Indolences',\n",
       " 128: 'It',\n",
       " 129: \"It's\",\n",
       " 130: 'Jack',\n",
       " 131: 'Jack!',\n",
       " 132: \"Jack's\",\n",
       " 133: 'Jack,',\n",
       " 134: 'Jacobolus',\n",
       " 135: 'January',\n",
       " 136: 'Jove',\n",
       " 137: 'Jove!\"',\n",
       " 138: 'Jove,',\n",
       " 139: 'Just',\n",
       " 140: 'KABALINI',\n",
       " 141: 'Lord',\n",
       " 142: 'Made',\n",
       " 143: 'May',\n",
       " 144: 'Miss',\n",
       " 145: 'Monte',\n",
       " 146: 'Monte4Carlo,',\n",
       " 147: 'Moon-dancers\"',\n",
       " 148: 'Mr',\n",
       " 149: 'Mrs.',\n",
       " 150: 'My',\n",
       " 151: 'No',\n",
       " 152: 'Now',\n",
       " 153: 'Nutley',\n",
       " 154: 'Of',\n",
       " 155: 'On',\n",
       " 156: 'Only',\n",
       " 157: 'Pathosbot',\n",
       " 158: 'PatríciaR',\n",
       " 159: 'Perhaps.',\n",
       " 160: 'Poor',\n",
       " 161: 'Professional',\n",
       " 162: 'Reisio',\n",
       " 163: 'Renaissance',\n",
       " 164: 'Rickham',\n",
       " 165: 'Rickham!',\n",
       " 166: 'Rickham,',\n",
       " 167: 'Rickham;',\n",
       " 168: 'Riviera',\n",
       " 169: 'Riviera,',\n",
       " 170: 'Riviera.',\n",
       " 171: 'Rocket000',\n",
       " 172: 'Rome',\n",
       " 173: 'Russian',\n",
       " 174: 'Sevres',\n",
       " 175: 'She',\n",
       " 176: \"She's\",\n",
       " 177: 'States',\n",
       " 178: 'Steinsplitter',\n",
       " 179: 'Strou',\n",
       " 180: 'Stroud',\n",
       " 181: 'Stroud!\"',\n",
       " 182: \"Stroud's\",\n",
       " 183: 'Stroud.',\n",
       " 184: 'Strouds',\n",
       " 185: 'Strouds.',\n",
       " 186: 'Suddenly',\n",
       " 187: 'Technion',\n",
       " 188: 'Tene~commonswiki',\n",
       " 189: 'That',\n",
       " 190: \"That's\",\n",
       " 191: 'The',\n",
       " 192: 'Then',\n",
       " 193: 'There',\n",
       " 194: 'There:',\n",
       " 195: 'They',\n",
       " 196: 'This',\n",
       " 197: 'Those',\n",
       " 198: 'Though',\n",
       " 199: 'Thwing',\n",
       " 200: \"Thwing's\",\n",
       " 201: 'Thwings',\n",
       " 202: 'To',\n",
       " 203: 'United',\n",
       " 204: 'Unported',\n",
       " 205: 'Usually',\n",
       " 206: 'Verdict',\n",
       " 207: 'Victor',\n",
       " 208: 'Was',\n",
       " 209: 'We',\n",
       " 210: 'Well',\n",
       " 211: 'Well!',\n",
       " 212: 'Well,',\n",
       " 213: 'Wharton',\n",
       " 214: 'What',\n",
       " 215: 'When',\n",
       " 216: 'Why',\n",
       " 217: 'Wikisource',\n",
       " 218: 'Y',\n",
       " 219: 'Yes',\n",
       " 220: 'Yes,',\n",
       " 221: 'You',\n",
       " 222: 'Zhaladshar',\n",
       " 223: 'Zigzig20s~enwikisource',\n",
       " 224: '_',\n",
       " 225: '_I',\n",
       " 226: '_am_',\n",
       " 227: '_famille-verte_',\n",
       " 228: '_felt_',\n",
       " 229: '_has_',\n",
       " 230: '_have_',\n",
       " 231: '_jardiniere_',\n",
       " 232: '_mine_',\n",
       " 233: '_not_',\n",
       " 234: '_rose',\n",
       " 235: '_rs_',\n",
       " 236: '_that_',\n",
       " 237: '_the_',\n",
       " 238: '_was',\n",
       " 239: '_was_',\n",
       " 240: '_were_',\n",
       " 241: 'a',\n",
       " 242: 'a9smile',\n",
       " 243: 'abdication.',\n",
       " 244: 'able',\n",
       " 245: 'about',\n",
       " 246: 'about;',\n",
       " 247: 'above',\n",
       " 248: 'abruptly',\n",
       " 249: 'absolute',\n",
       " 250: 'absorbed',\n",
       " 251: 'absurdity',\n",
       " 252: 'academic',\n",
       " 253: 'accessible',\n",
       " 254: 'accuse',\n",
       " 255: 'accustomed',\n",
       " 256: 'across',\n",
       " 257: 'activity',\n",
       " 258: 'add,',\n",
       " 259: 'added',\n",
       " 260: 'admirers',\n",
       " 261: 'adopted',\n",
       " 262: 'adulation.',\n",
       " 263: 'advance,',\n",
       " 264: 'aesthetic',\n",
       " 265: 'affect',\n",
       " 266: 'afraid',\n",
       " 267: 'after',\n",
       " 268: 'after.',\n",
       " 269: 'afterward',\n",
       " 270: 'again',\n",
       " 271: 'again\"?',\n",
       " 272: 'again,',\n",
       " 273: 'again.',\n",
       " 274: 'again?\"',\n",
       " 275: 'ago,\"',\n",
       " 276: 'ah,',\n",
       " 277: 'air',\n",
       " 278: 'alive',\n",
       " 279: 'alive.',\n",
       " 280: 'all',\n",
       " 281: 'all,',\n",
       " 282: 'almost',\n",
       " 283: 'alone',\n",
       " 284: 'alone,',\n",
       " 285: 'along',\n",
       " 286: 'always',\n",
       " 287: 'amazement',\n",
       " 288: 'amid',\n",
       " 289: 'among',\n",
       " 290: 'amplest',\n",
       " 291: 'amusing',\n",
       " 292: 'an',\n",
       " 293: 'and',\n",
       " 294: 'and,',\n",
       " 295: 'and15thought',\n",
       " 296: 'another',\n",
       " 297: 'answer',\n",
       " 298: 'answered',\n",
       " 299: 'any',\n",
       " 300: 'anything',\n",
       " 301: 'anywhere',\n",
       " 302: 'apparent',\n",
       " 303: 'apparently',\n",
       " 304: 'appearance',\n",
       " 305: 'appeared',\n",
       " 306: 'apply',\n",
       " 307: 'appointed',\n",
       " 308: 'are',\n",
       " 309: 'areas',\n",
       " 310: 'arm',\n",
       " 311: 'arm-chair',\n",
       " 312: 'arm-chairs',\n",
       " 313: 'arms',\n",
       " 314: 'art',\n",
       " 315: 'art,',\n",
       " 316: 'art.\"19',\n",
       " 317: 'articles',\n",
       " 318: 'artist',\n",
       " 319: 'as',\n",
       " 320: 'aside',\n",
       " 321: 'asked',\n",
       " 322: 'asked,',\n",
       " 323: 'at',\n",
       " 324: 'atmosphere',\n",
       " 325: 'atom',\n",
       " 326: 'attack.',\n",
       " 327: 'attention',\n",
       " 328: 'attention;',\n",
       " 329: 'attitude',\n",
       " 330: 'audacities,',\n",
       " 331: 'away',\n",
       " 332: 'awful',\n",
       " 333: 'axioms',\n",
       " 334: 'azaleas,',\n",
       " 335: 'back',\n",
       " 336: 'background',\n",
       " 337: 'balance',\n",
       " 338: 'balancing,',\n",
       " 339: 'balustraded',\n",
       " 340: 'basking',\n",
       " 341: 'bath-rooms,',\n",
       " 342: 'be',\n",
       " 343: 'be.',\n",
       " 344: 'beaming',\n",
       " 345: 'bean-stalk.',\n",
       " 346: 'bear',\n",
       " 347: 'beard',\n",
       " 348: 'beard,',\n",
       " 349: 'beauty',\n",
       " 350: 'became',\n",
       " 351: 'because',\n",
       " 352: 'becoming',\n",
       " 353: 'bed.',\n",
       " 354: 'been',\n",
       " 355: 'been,',\n",
       " 356: 'before',\n",
       " 357: 'before,',\n",
       " 358: 'began',\n",
       " 359: 'began,',\n",
       " 360: 'begun',\n",
       " 361: 'behind',\n",
       " 362: 'behind.',\n",
       " 363: 'being',\n",
       " 364: 'belie',\n",
       " 365: 'believed',\n",
       " 366: 'beneath',\n",
       " 367: 'bespoke',\n",
       " 368: 'better',\n",
       " 369: 'better;',\n",
       " 370: 'between',\n",
       " 371: 'between.',\n",
       " 372: 'big',\n",
       " 373: 'bits',\n",
       " 374: 'bitterness,',\n",
       " 375: 'blocked',\n",
       " 376: 'book,',\n",
       " 377: 'book:',\n",
       " 378: 'books',\n",
       " 379: 'born',\n",
       " 380: 'borne',\n",
       " 381: 'boudoir',\n",
       " 382: 'bravura',\n",
       " 383: 'break',\n",
       " 384: 'breaking',\n",
       " 385: 'breathing',\n",
       " 386: 'bric-a-brac,',\n",
       " 387: 'briefly',\n",
       " 388: 'bronzes',\n",
       " 389: 'brought',\n",
       " 390: 'brown',\n",
       " 391: 'brush',\n",
       " 392: 'brush.\"',\n",
       " 393: 'built',\n",
       " 394: 'bull',\n",
       " 395: 'business',\n",
       " 396: 'but',\n",
       " 397: 'buying',\n",
       " 398: 'by',\n",
       " 399: 'called',\n",
       " 400: 'came',\n",
       " 401: 'can',\n",
       " 402: 'can?',\n",
       " 403: 'canvas',\n",
       " 404: 'canvases,',\n",
       " 405: 'cards.',\n",
       " 406: 'care',\n",
       " 407: 'career',\n",
       " 408: 'caught',\n",
       " 409: 'centra',\n",
       " 410: 'chair',\n",
       " 411: 'chap,',\n",
       " 412: 'characteristic',\n",
       " 413: 'charming,',\n",
       " 414: 'cheap',\n",
       " 415: 'check.',\n",
       " 416: 'cheeks',\n",
       " 417: 'chest',\n",
       " 418: 'chimney-piece.',\n",
       " 419: 'choice,',\n",
       " 420: 'chucked',\n",
       " 421: 'cigar',\n",
       " 422: 'cigarette',\n",
       " 423: 'cigars',\n",
       " 424: 'circulation,\"',\n",
       " 425: 'circumstance',\n",
       " 426: \"circus-clown's\",\n",
       " 427: 'claimed',\n",
       " 428: 'clasping',\n",
       " 429: 'clear',\n",
       " 430: 'cleverer',\n",
       " 431: 'close',\n",
       " 432: 'clue',\n",
       " 433: 'coat,',\n",
       " 434: 'collapsed',\n",
       " 435: 'collection',\n",
       " 436: 'colou',\n",
       " 437: 'come',\n",
       " 438: 'comes',\n",
       " 439: 'comfortable',\n",
       " 440: 'coming',\n",
       " 441: 'commercial',\n",
       " 442: 'committed',\n",
       " 443: 'companion',\n",
       " 444: 'compared',\n",
       " 445: 'complex',\n",
       " 446: 'confident',\n",
       " 447: 'congesting',\n",
       " 448: 'conjugal',\n",
       " 449: 'constantly',\n",
       " 450: 'constraint',\n",
       " 451: 'consummate',\n",
       " 452: 'contende',\n",
       " 453: 'continued',\n",
       " 454: 'contributed',\n",
       " 455: 'copyright',\n",
       " 456: 'copyrighted',\n",
       " 457: 'corner',\n",
       " 458: 'corrected',\n",
       " 459: 'could',\n",
       " 460: \"couldn't\",\n",
       " 461: 'count,',\n",
       " 462: 'countenance.',\n",
       " 463: 'countries',\n",
       " 464: 'coup',\n",
       " 465: 'course',\n",
       " 466: 'course,',\n",
       " 467: 'covered',\n",
       " 468: 'craft',\n",
       " 469: 'cried.',\n",
       " 470: 'crossed',\n",
       " 471: 'crowned',\n",
       " 472: 'crumbled.',\n",
       " 473: 'cry',\n",
       " 474: 'cured',\n",
       " 475: 'curiosity',\n",
       " 476: 'curious',\n",
       " 477: 'current',\n",
       " 478: 'curtain',\n",
       " 479: 'curtains',\n",
       " 480: 'd',\n",
       " 481: 'd!',\n",
       " 482: 'd,',\n",
       " 483: 'dabble',\n",
       " 484: 'damask',\n",
       " 485: 'dark',\n",
       " 486: 'dashed',\n",
       " 487: 'day',\n",
       " 488: 'day,',\n",
       " 489: 'days',\n",
       " 490: 'days,',\n",
       " 491: 'dead',\n",
       " 492: 'dead.\"',\n",
       " 493: 'dead?',\n",
       " 494: 'deadening',\n",
       " 495: 'dear',\n",
       " 496: 'dear,',\n",
       " 497: 'deep',\n",
       " 498: \"deerhound's\",\n",
       " 499: 'degree',\n",
       " 500: 'delicate',\n",
       " 501: 'demand',\n",
       " 502: 'denied',\n",
       " 503: 'deploring',\n",
       " 504: 'deprecating',\n",
       " 505: 'deprecatingly',\n",
       " 506: 'desire',\n",
       " 507: 'destroyed',\n",
       " 508: 'destructio',\n",
       " 509: 'desultory',\n",
       " 510: 'detail.',\n",
       " 511: 'developing',\n",
       " 512: 'diagnosis',\n",
       " 513: 'did',\n",
       " 514: 'did.',\n",
       " 515: \"didn't\",\n",
       " 516: 'died',\n",
       " 517: 'digital',\n",
       " 518: 'dim,',\n",
       " 519: 'dimmest',\n",
       " 520: 'dingy',\n",
       " 521: 'dining-room.',\n",
       " 522: 'disarming,',\n",
       " 523: 'discovery;',\n",
       " 524: 'discrimination',\n",
       " 525: 'discussion',\n",
       " 526: 'disdain',\n",
       " 527: 'disdained',\n",
       " 528: 'diseas',\n",
       " 529: 'disguised',\n",
       " 530: 'display',\n",
       " 531: 'dissatisfied',\n",
       " 532: 'distinguished',\n",
       " 533: 'distract',\n",
       " 534: 'distribute',\n",
       " 535: 'divert',\n",
       " 536: 'do',\n",
       " 537: \"doesn't\",\n",
       " 538: 'doing',\n",
       " 539: 'domain',\n",
       " 540: 'domestic',\n",
       " 541: \"don't\",\n",
       " 542: 'done',\n",
       " 543: 'donkey',\n",
       " 544: 'down',\n",
       " 545: 'down,',\n",
       " 546: 'down.\"',\n",
       " 547: 'dozen',\n",
       " 548: 'dragged',\n",
       " 549: 'drawing-room',\n",
       " 550: 'drawing-rooms',\n",
       " 551: 'drawn',\n",
       " 552: 'dress-closets,',\n",
       " 553: 'drew',\n",
       " 554: 'dropped',\n",
       " 555: 'e,',\n",
       " 556: 'e-book',\n",
       " 557: 'e-books',\n",
       " 558: 'each',\n",
       " 559: 'earth',\n",
       " 560: 'ease',\n",
       " 561: 'easel',\n",
       " 562: 'easel.',\n",
       " 563: 'easy',\n",
       " 564: 'eath.',\n",
       " 565: 'echoed',\n",
       " 566: 'economy',\n",
       " 567: 'edition',\n",
       " 568: 'effect',\n",
       " 569: 'effects\";',\n",
       " 570: 'efforts',\n",
       " 571: 'egregious',\n",
       " 572: 'eighteenth-century',\n",
       " 573: 'elbow',\n",
       " 574: 'elegant',\n",
       " 575: 'else,',\n",
       " 576: 'else.',\n",
       " 577: 'embarrassed',\n",
       " 578: 'en:',\n",
       " 579: 'enabled',\n",
       " 580: 'end',\n",
       " 581: 'endless',\n",
       " 582: 'enjoy',\n",
       " 583: 'enlightenment:',\n",
       " 584: 'enough',\n",
       " 585: 'enough,',\n",
       " 586: 'ensuing',\n",
       " 587: 'equally',\n",
       " 588: 'equanimity',\n",
       " 589: 'errors.',\n",
       " 590: 'ery:',\n",
       " 591: 'escape',\n",
       " 592: 'est,\"',\n",
       " 593: 'established',\n",
       " 594: 'etching?\"',\n",
       " 595: 'even',\n",
       " 596: 'event',\n",
       " 597: 'ever',\n",
       " 598: 'everlasting',\n",
       " 599: 'every',\n",
       " 600: 'exasperated',\n",
       " 601: 'except',\n",
       " 602: 'excuse',\n",
       " 603: 'excusing',\n",
       " 604: 'existed',\n",
       " 605: 'expected',\n",
       " 606: 'exploitation),',\n",
       " 607: 'exquisite',\n",
       " 608: 'exquisitely',\n",
       " 609: 'extenuation.',\n",
       " 610: 'exterminating',\n",
       " 611: 'extracting',\n",
       " 612: 'eye',\n",
       " 613: 'eyebrows',\n",
       " 614: 'eyes',\n",
       " 615: 'eyes.',\n",
       " 616: 'eyes:',\n",
       " 617: 'face',\n",
       " 618: 'face,',\n",
       " 619: 'faces.',\n",
       " 620: 'fact',\n",
       " 621: 'fact,',\n",
       " 622: 'faded',\n",
       " 623: 'failed',\n",
       " 624: 'failure',\n",
       " 625: 'failure,',\n",
       " 626: 'fair',\n",
       " 627: 'faith,',\n",
       " 628: 'false',\n",
       " 629: 'familiar',\n",
       " 630: 'fancy',\n",
       " 631: 'fashionable',\n",
       " 632: 'fate',\n",
       " 633: 'feather',\n",
       " 634: 'feet,',\n",
       " 635: 'fell',\n",
       " 636: 'fellow',\n",
       " 637: 'felt',\n",
       " 638: 'few',\n",
       " 639: 'fewer',\n",
       " 640: 'finality',\n",
       " 641: 'find',\n",
       " 642: 'fingers',\n",
       " 643: 'first',\n",
       " 644: 'first,',\n",
       " 645: 'fit',\n",
       " 646: 'fitting',\n",
       " 647: 'five',\n",
       " 648: 'flash',\n",
       " 649: 'flashed',\n",
       " 650: 'florid',\n",
       " 651: 'flowers',\n",
       " 652: 'fluently',\n",
       " 653: 'flung',\n",
       " 654: 'follow',\n",
       " 655: 'followed',\n",
       " 656: 'following',\n",
       " 657: 'fond',\n",
       " 658: 'footstep',\n",
       " 659: 'for',\n",
       " 660: 'forced',\n",
       " 661: 'forcing',\n",
       " 662: 'forehead,',\n",
       " 663: 'foreign',\n",
       " 664: 'foreseen',\n",
       " 665: 'forgive',\n",
       " 666: 'forgotten',\n",
       " 667: 'form',\n",
       " 668: 'formed',\n",
       " 669: 'forming,',\n",
       " 670: 'forward',\n",
       " 671: 'fostered',\n",
       " 672: 'found',\n",
       " 673: 'foundations',\n",
       " 674: 'foundations,',\n",
       " 675: 'foundations.',\n",
       " 676: 'fragment',\n",
       " 677: 'fragments.',\n",
       " 678: 'frame',\n",
       " 679: 'frame.',\n",
       " 680: 'frames.',\n",
       " 681: 'free',\n",
       " 682: 'free,',\n",
       " 683: 'frequently',\n",
       " 684: \"friend's\",\n",
       " 685: 'from',\n",
       " 686: 'full',\n",
       " 687: 'fullest',\n",
       " 688: 'furiously',\n",
       " 689: 'furrowed',\n",
       " 690: 'g',\n",
       " 691: 'gan',\n",
       " 692: 'garlanded',\n",
       " 693: 'garlands',\n",
       " 694: 'gave',\n",
       " 695: 'genial',\n",
       " 696: 'genius',\n",
       " 697: 'gesture,',\n",
       " 698: 'get',\n",
       " 699: 'getting',\n",
       " 700: 'give',\n",
       " 701: 'given',\n",
       " 702: 'glad',\n",
       " 703: 'glanced',\n",
       " 704: 'glimpse',\n",
       " 705: 'gloried',\n",
       " 706: 'glory',\n",
       " 707: 'glory\"',\n",
       " 708: 'go',\n",
       " 709: 'going',\n",
       " 710: 'gone',\n",
       " 711: 'good',\n",
       " 712: 'good-breeding,',\n",
       " 713: 'good-humoured',\n",
       " 714: 'got',\n",
       " 715: 'grace',\n",
       " 716: 'gradually',\n",
       " 717: 'gray',\n",
       " 718: 'grayish',\n",
       " 719: 'great',\n",
       " 720: 'greatest',\n",
       " 721: 'greatness',\n",
       " 722: 'grew',\n",
       " 723: 'groping',\n",
       " 724: 'growing',\n",
       " 725: 'had',\n",
       " 726: \"hadn't\",\n",
       " 727: 'hair',\n",
       " 728: 'half',\n",
       " 729: 'half-light,',\n",
       " 730: 'half-mechanically',\n",
       " 731: 'hall,',\n",
       " 732: 'hand',\n",
       " 733: 'hand,',\n",
       " 734: 'hands',\n",
       " 735: 'handsome',\n",
       " 736: 'handsome,',\n",
       " 737: 'hanging',\n",
       " 738: 'happe',\n",
       " 739: 'happen',\n",
       " 740: 'happened',\n",
       " 741: 'happened,\"',\n",
       " 742: 'happened?',\n",
       " 743: 'hard',\n",
       " 744: 'hardly',\n",
       " 745: 'have',\n",
       " 746: \"haven't\",\n",
       " 747: 'having',\n",
       " 748: 'having,',\n",
       " 749: 'he',\n",
       " 750: \"he'd\",\n",
       " 751: \"he's\",\n",
       " 752: 'head',\n",
       " 753: 'head,',\n",
       " 754: 'hear',\n",
       " 755: 'heard',\n",
       " 756: 'heard,',\n",
       " 757: 'heart',\n",
       " 758: 'height',\n",
       " 759: 'her',\n",
       " 760: 'her.',\n",
       " 761: 'here',\n",
       " 762: 'here;',\n",
       " 763: 'hermit.\"',\n",
       " 764: 'herself',\n",
       " 765: 'herself.',\n",
       " 766: 'hesitations',\n",
       " 767: 'hide',\n",
       " 768: 'high',\n",
       " 769: 'him',\n",
       " 770: 'him!',\n",
       " 771: 'him,',\n",
       " 772: 'him.',\n",
       " 773: 'him:',\n",
       " 774: 'himself',\n",
       " 775: 'himself,',\n",
       " 776: 'himself,\"',\n",
       " 777: 'hint',\n",
       " 778: 'his',\n",
       " 779: 'his!',\n",
       " 780: 'history',\n",
       " 781: 'history?\"',\n",
       " 782: 'holding',\n",
       " 783: 'home',\n",
       " 784: 'honour',\n",
       " 785: 'hooded',\n",
       " 786: 'hostess',\n",
       " 787: 'hostess:',\n",
       " 788: 'hot-house',\n",
       " 789: 'hour.',\n",
       " 790: 'hours,',\n",
       " 791: 'house',\n",
       " 792: 'house.\"',\n",
       " 793: 'how',\n",
       " 794: \"how'?\",\n",
       " 795: 'how,',\n",
       " 796: 'hung',\n",
       " 797: 'husband',\n",
       " 798: \"husband's\",\n",
       " 799: 'husband,',\n",
       " 800: 'idea',\n",
       " 801: 'idle',\n",
       " 802: 'idling',\n",
       " 803: 'if',\n",
       " 804: 'immediately',\n",
       " 805: 'in',\n",
       " 806: 'incense.',\n",
       " 807: 'including',\n",
       " 808: 'indifferent;',\n",
       " 809: 'inevitable',\n",
       " 810: 'inevitably',\n",
       " 811: 'inflexible',\n",
       " 812: 'ing',\n",
       " 813: 'insensible',\n",
       " 814: 'insignificant',\n",
       " 815: 'instinctively',\n",
       " 816: 'instructive',\n",
       " 817: 'interesting',\n",
       " 818: 'interesting\":',\n",
       " 819: 'into',\n",
       " 820: 'ion',\n",
       " 821: 'ironic',\n",
       " 822: 'irony',\n",
       " 823: 'irrelevance',\n",
       " 824: 'irrevocable',\n",
       " 825: 'is',\n",
       " 826: 'is,',\n",
       " 827: 'it',\n",
       " 828: \"it's\",\n",
       " 829: 'it,',\n",
       " 830: 'it,\"',\n",
       " 831: 'it.',\n",
       " 832: 'it.\"',\n",
       " 833: 'it;',\n",
       " 834: 'it?\"',\n",
       " 835: 'its',\n",
       " 836: 'itself',\n",
       " 837: 'jealousy',\n",
       " 838: 'jealousy?',\n",
       " 839: 'just',\n",
       " 840: 'keep',\n",
       " 841: 'kept',\n",
       " 842: 'kind',\n",
       " 843: 'kind:',\n",
       " 844: 'knees.',\n",
       " 845: 'knew',\n",
       " 846: 'knew?',\n",
       " 847: 'know',\n",
       " 848: 'know;',\n",
       " 849: 'known_.',\n",
       " 850: 'l',\n",
       " 851: 'l-piece',\n",
       " 852: 'laid',\n",
       " 853: 'lair,\"',\n",
       " 854: 'landing.',\n",
       " 855: 'language,',\n",
       " 856: 'last',\n",
       " 857: 'late',\n",
       " 858: 'later',\n",
       " 859: 'latter',\n",
       " 860: \"latter's\",\n",
       " 861: 'laugh',\n",
       " 862: 'laugh.',\n",
       " 863: 'laugh:',\n",
       " 864: 'laughed',\n",
       " 865: 'lay',\n",
       " 866: 'le',\n",
       " 867: 'leading',\n",
       " 868: 'lean',\n",
       " 869: 'learned',\n",
       " 870: 'least',\n",
       " 871: 'leath',\n",
       " 872: 'leave',\n",
       " 873: 'led',\n",
       " 874: 'left',\n",
       " 875: 'leisure!\"',\n",
       " 876: 'lends',\n",
       " 877: 'lent',\n",
       " 878: 'let',\n",
       " 879: 'letters...',\n",
       " 880: 'library',\n",
       " 881: 'license',\n",
       " 882: 'license.',\n",
       " 883: 'lies!',\n",
       " 884: 'life',\n",
       " 885: 'life,',\n",
       " 886: 'life-likeness',\n",
       " 887: 'life.',\n",
       " 888: 'lift',\n",
       " 889: 'lifted',\n",
       " 890: 'light',\n",
       " 891: 'light.',\n",
       " 892: 'lightly;',\n",
       " 893: 'like',\n",
       " 894: 'like.\"',\n",
       " 895: 'liked',\n",
       " 896: 'line.',\n",
       " 897: 'lines',\n",
       " 898: 'lingered',\n",
       " 899: 'lips,',\n",
       " 900: 'lit',\n",
       " 901: 'little',\n",
       " 902: 'little:',\n",
       " 903: 'live',\n",
       " 904: 'loathing',\n",
       " 905: 'long',\n",
       " 906: 'longed',\n",
       " 907: 'longer',\n",
       " 908: 'look',\n",
       " 909: 'looked',\n",
       " 910: 'lookin',\n",
       " 911: 'looking',\n",
       " 912: 'looking,\"',\n",
       " 913: 'lose',\n",
       " 914: 'loss',\n",
       " 915: 'loungi',\n",
       " 916: 'lovely',\n",
       " 917: 'lucky',\n",
       " 918: 'lump',\n",
       " 919: 'luncheon-table,',\n",
       " 920: 'luxury',\n",
       " 921: 'lying',\n",
       " 922: 'made',\n",
       " 923: 'magazines,',\n",
       " 924: 'make',\n",
       " 925: 'man',\n",
       " 926: 'man,',\n",
       " 927: 'manage',\n",
       " 928: 'managed',\n",
       " 929: 'mante',\n",
       " 930: 'mantel-piece,',\n",
       " 931: 'marble',\n",
       " 932: 'married',\n",
       " 933: 'may',\n",
       " 934: 'me',\n",
       " 935: 'me!',\n",
       " 936: 'me!\"',\n",
       " 937: 'me,',\n",
       " 938: 'me.',\n",
       " 939: 'me.\"',\n",
       " 940: 'meant',\n",
       " 941: 'mediocrity\"',\n",
       " 942: 'medium',\n",
       " 943: 'members.',\n",
       " 944: 'mentione',\n",
       " 945: 'mere',\n",
       " 946: 'merely',\n",
       " 947: 'met',\n",
       " 948: 'met,',\n",
       " 949: 'might',\n",
       " 950: 'mighty',\n",
       " 951: \"millionaire's\",\n",
       " 952: 'mine,',\n",
       " 953: 'mine:',\n",
       " 954: 'minute,',\n",
       " 955: 'minutes',\n",
       " 956: 'mirrors.',\n",
       " 957: 'modest',\n",
       " 958: 'modesty',\n",
       " 959: 'moment',\n",
       " 960: 'money',\n",
       " 961: 'monumental',\n",
       " 962: 'mood',\n",
       " 963: 'morbidly',\n",
       " 964: 'more',\n",
       " 965: 'more,',\n",
       " 966: 'more.',\n",
       " 967: 'more?\"',\n",
       " 968: 'most',\n",
       " 969: 'mourn',\n",
       " 970: 'mourned.',\n",
       " 971: 'moustache,',\n",
       " 972: 'moved',\n",
       " 973: 'moved,',\n",
       " 974: 'moved.',\n",
       " 975: 'much',\n",
       " 976: 'muddling;',\n",
       " 977: 'multilingual',\n",
       " 978: 'multiplied',\n",
       " 979: 'murmur',\n",
       " 980: 'muscles,',\n",
       " 981: 'must',\n",
       " 982: 'my',\n",
       " 983: 'myself',\n",
       " 984: 'mysterious',\n",
       " 985: 'n',\n",
       " 986: 'n.',\n",
       " 987: 'naive',\n",
       " 988: 'native',\n",
       " 989: 'near',\n",
       " 990: 'nearly',\n",
       " 991: 'negatived',\n",
       " 992: 'nervous',\n",
       " 993: 'nervousness;',\n",
       " 994: 'neutral',\n",
       " 995: 'never',\n",
       " 996: 'new',\n",
       " 997: 'next',\n",
       " 998: 'ng',\n",
       " 999: 'no',\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "w = sorted(set(preprocess))\n",
    "vocabulary = {key: value for key, value in enumerate(w)}\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb157a-2620-49e0-aa1e-239c9a49aba5",
   "metadata": {},
   "source": [
    "# Encode and Decode \n",
    "## Endcode: Simple Text -> Token text -> Token ID\n",
    "1. Simple Text (Raw Input):   \"Hello, world!\"\n",
    "2. Tokenized Text (Splitting into Tokens):\n",
    "\n",
    "               A- Whole words (\"Hello\", \"world\")\n",
    "               B- Subwords (\"unhappiness\" → \"un\", \"happiness\")\n",
    "               C- Punctuation and symbols (\",\", \"!\")\n",
    "               Tokenized Output: [\"Hello\", \",\", \"world\", \"!\"]\n",
    "3. Token IDs (Numerical Representation):\n",
    "\n",
    "                \"Hello\" → 15496\n",
    "                \",\" → 11\n",
    "                \"world\" → 995\n",
    "                \"!\" → 0\n",
    "                Final Token IDs: [15496, 11, 995, 0]\n",
    "## Decode: Token ID -> Token Text -> Simple Text\n",
    "    \n",
    "    Decode is the reverse: Token id -> Token Text -> Simple Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdb4406e-8c9c-4a35-9371-b76a749caa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenization:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.string_to_int = vocabulary\n",
    "        self.int_to_string = {item: s for s,item in vocabulary.items()}\n",
    "\n",
    "    def encoding(self, text):\n",
    "        # split text around empty space, comman and period\n",
    "        preprocessing = re.split(r'([,.:?!\",()]|--|\\s)', text)\n",
    "        # Tokenize by removing all space, comma, --, etcc\n",
    "        preprocessing = [word.strip() for word in preprocessing if word.strip() ]\n",
    "        # converting into id number\n",
    "        id = [self.string_to_int[s] for s in preprocessing ]\n",
    "        return id \n",
    "\n",
    "    def decoding(self, vocabulary ):\n",
    "        # convert token id to Token text\n",
    "        tokenID_to_TokenText = \" \".join([self.int_to_string[i] for i in id])\n",
    "        # replace empty space with punctuation\n",
    "        tokenID_to_TokenText =  re.sub (r'([,.:?!\",()]|--|\\s)', r'\\1', tokenID_to_TokenText)\n",
    "        return tokenID_to_TokenText\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02488619-bf3a-4606-8f19-67f56f660845",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'height'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m Tok = Tokenization(vocabulary)\n\u001b[32m      2\u001b[39m inputText = \u001b[33m\"\"\"\u001b[39m\u001b[33mheight of his glory that was what the women called it.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m tokenID = \u001b[43mTok\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputText\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenID)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mTokenization.encoding\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     10\u001b[39m preprocessing = [word.strip() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m preprocessing \u001b[38;5;28;01mif\u001b[39;00m word.strip() ]\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# converting into id number\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mid\u001b[39m = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstring_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessing ]\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mid\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'height'"
     ]
    }
   ],
   "source": [
    "# unseen word give us error because of short data\n",
    "Tok = Tokenization(vocabulary)\n",
    "inputText = \"\"\"height of his glory that was what the women called it.\"\"\"\n",
    "tokenID = Tok.encoding(inputText)\n",
    "print(tokenID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376c2bab-9505-4b61-b102-03a17824f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to deal with unseen token\n",
    "# Assume preprocessing is your list of tokens from dataset\n",
    "unseen_token = sorted(list(set(preprocessing)))\n",
    "\n",
    "# Add special tokens explicitly\n",
    "unseen_token.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "# Assign each token an integer ID\n",
    "vocab = {tok: num for num, tok in enumerate(unseen_token)}\n",
    "\n",
    "# Vocabulary size\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942e1b4-b666-4e7f-bb2c-2a59d3a4a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When your model encounters a word not in vocab, you map it to <|unk|>:\n",
    "def get_token_id(token, vocab):\n",
    "    return vocab.get(token, vocab[\"<|unk|>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef41a3e3-6f7d-4a4b-9365-d923c63349f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.11.0-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2025.9.1-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\awalehdek\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\awalehdek\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\awalehdek\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\awalehdek\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\awalehdek\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
      "Downloading tiktoken-0.11.0-cp313-cp313-win_amd64.whl (883 kB)\n",
      "   ---------------------------------------- 0.0/883.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/883.9 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 524.3/883.9 kB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 786.4/883.9 kB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 883.9/883.9 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading regex-2025.9.1-cp313-cp313-win_amd64.whl (275 kB)\n",
      "Installing collected packages: regex, tiktoken\n",
      "\n",
      "   ---------------------------------------- 0/2 [regex]\n",
      "   ---------------------------------------- 0/2 [regex]\n",
      "   ---------------------------------------- 0/2 [regex]\n",
      "   ---------------------------------------- 0/2 [regex]\n",
      "   ---------------------------------------- 0/2 [regex]\n",
      "   -------------------- ------------------- 1/2 [tiktoken]\n",
      "   -------------------- ------------------- 1/2 [tiktoken]\n",
      "   -------------------- ------------------- 1/2 [tiktoken]\n",
      "   -------------------- ------------------- 1/2 [tiktoken]\n",
      "   ---------------------------------------- 2/2 [tiktoken]\n",
      "\n",
      "Successfully installed regex-2025.9.1 tiktoken-0.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0becd7e4-0095-4f49-959d-e628d0f81809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3eee3-4daa-4ca3-8fcc-52fead0a512e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
