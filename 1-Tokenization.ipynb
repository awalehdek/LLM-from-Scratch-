{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa873a4-2d25-48d2-8096-0efc93eaee16",
   "metadata": {},
   "source": [
    "# Create Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99648962-cc52-4239-9975-1d19a7ba474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\awalehdek\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84f87e66-f7f2-4731-a93b-8622311b8abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 21918\n",
      "1The Verdict\n",
      "Edith Wharton\n",
      "1908\n",
      "Exported from Wikisource on May 20, 20242I HAD always thought Jack Gisburn rather a cheap genius--\n",
      "though a good fellow enough--so it was no great surprise to\n",
      "me to hear that, in the height of his glory , he had dropped\n",
      "his painting, married a rich widow , and established himself\n",
      "in a villa on the Riviera. (Though I rather thought it would\n",
      "have been Rome or Florence.)\n",
      "\"The height of his glory\"--that was what the women called\n",
      "it. I can hear Mrs. Gideon Thwing--his last Chicago sitter --\n",
      "deploring his unaccountable abdication. \"Of course it's\n",
      "going to send the value of my picture 'way up; but I don't\n",
      "think of that, Mr . Rickham--the loss to Arrt is all I think of.\"\n",
      "The word, on Mrs. Thwing's lips, multiplied its _rs_ as\n",
      "though they were reflected in an endless vista of mirrors.\n",
      "And it was not only the Mrs. Thwings who mourned. Had\n",
      "not the exquisite  Hermia Croft, at the last Grafton Gallery\n",
      "show , stopped me before Gisburn's \"Moon-dancers\" to say,\n",
      "with te\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "# Open in binary mode\n",
    "with open(\"The_Verdict.pdf\", \"rb\") as fp:\n",
    "    reader = PyPDF2.PdfReader(fp)\n",
    "    text_book = \"\"\n",
    "    for page in reader.pages:\n",
    "        text_book += page.extract_text() or \"\"  # Avoid None\n",
    "\n",
    "print(\"Total number of characters:\", len(text_book))\n",
    "print(text_book[:999])\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4559df-768b-4ff0-9c71-14a94796a276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today', 'is', 'sunny,', \"let's\", '--', 'go', 'to', 'the', 'beach!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "text1  = \"Today is sunny, let's -- go to the beach!\"\n",
    "# split text around empty space\n",
    "res = re.split(r'\\s', text1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72fec4c2-97f7-4b6c-9d4e-247fb15ba94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today', 'is', 'sunny', '', \"let's\", '--', 'go', 'to', 'the', 'beach!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split text around empty space, comman and period\n",
    "res = re.split(r'[,.]|\\s', text1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "424b31bd-cbfb-4e44-bff2-102a905ad07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: ['Today', 'is', 'sunny', ',', \"let's\", '--', 'go', 'to', 'the', 'beach', '!']\n",
      "res1: ['Today', 'is', 'sunny', ',', \"let's\", '--', 'go', 'to', 'the', 'beach', '!']\n"
     ]
    }
   ],
   "source": [
    "res = re.split(r'([,.:?!\",()]|--|\\s)', text1)\n",
    "\n",
    "# Removing white spaces\n",
    "res = [word for word in res if word.strip() ]\n",
    "print(\"res:\", res)\n",
    "\n",
    "# Alternative: Keep words AND punctuation as separate tokens but no empty strings\n",
    "res1 =  re.findall(r'[,.:?!\",()]|--|[^,.:?!\",()\\s]+', text1) \n",
    "print(\"res1:\", res1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9844461-650e-4080-8ffe-01b16e0b2290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: ['Today', 'is', 'sunny', ',', \"let's\", '--', 'go', 'to', 'the', 'beach', '!']\n",
      "res1: ['Today', 'is', 'sunny', \"let's\", 'go', 'to', 'the', 'beach']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awalehdek\\AppData\\Local\\Temp\\ipykernel_14556\\444435906.py:6: FutureWarning: Possible set difference at position 13\n",
      "  res1 = re.findall(r'[^\\s,.:?!\",()--]+|(?<=\\w)\\'(?=\\w)', text1)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize by removing all space, comma, --, etcc\n",
    "res = [word.strip() for word in res if word.strip() ]\n",
    "print(\"res:\",res)\n",
    "\n",
    "# If you want to keep apostrophes in words like \"let's\":\n",
    "res1 = re.findall(r'[^\\s,.:?!\",()--]+|(?<=\\w)\\'(?=\\w)', text1)\n",
    "print(\"res1:\", res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cfff15f-b80e-4d67-8ffd-54e6436d55a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess:\n",
      " ['1The', 'Verdict', 'Edith', 'Wharton', '1908', 'Exported', 'from', 'Wikisource', 'on', 'May', '20,', '20242I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap']\n",
      "preprocess_1:\n",
      " ['1The', 'Verdict', 'Edith', 'Wharton', '1908', 'Exported', 'from', 'Wikisource', 'on', 'May', '20,', '20242I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awalehdek\\AppData\\Local\\Temp\\ipykernel_14556\\823624902.py:10: FutureWarning: Possible set difference at position 13\n",
      "  preprocess_1 = re.findall(r'[^\\s,.:?!\",()--]+|[,.:?!\",()]|--', text_book)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing Case 1:\n",
    "# Split text on punctuation, symbols, or whitespace, keeping delimiters\n",
    "preprocess = re.split(r'( [,.:?!\",()]|--|\\s)', text_book)\n",
    "# Remove empty strings and whitespace-only elements\n",
    "preprocess = [word.strip() for word in preprocess if word.strip() ]\n",
    "print('preprocess:\\n',preprocess[:20])\n",
    "\n",
    "# preprocessing Case 2:\n",
    "# Alternative approaches Extract words and punctuation as separate tokens\n",
    "preprocess_1 = re.findall(r'[^\\s,.:?!\",()--]+|[,.:?!\",()]|--', text_book)\n",
    "# Extract all non-whitespace chunks (includes punctuation attached to words)\n",
    "preprocess_1 = re.findall(r'[^\\s]+', text_book)\n",
    "print('preprocess_1:\\n',preprocess_1[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb542d0-2630-421d-94ac-f54da2e29812",
   "metadata": {},
   "source": [
    "# get the unique word vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "365f6fea-efe6-4164-be18-12a84afaf4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vocabulary:_________\n",
      "0: !\n",
      "1: \"\n",
      "2: \"Ah\n",
      "3: \"Ah,\n",
      "4: \"Begin\n",
      "5: \"By\n",
      "6: \"Destroyed\n",
      "7: \"Don't\n",
      "8: \"Hang\n",
      "9: \"Has\n",
      "\n",
      "string_to_int:_____________\n",
      "!: 0\n",
      "\": 1\n",
      "\"Ah: 2\n",
      "\"Ah,: 3\n",
      "\"Begin: 4\n",
      "\"By: 5\n",
      "\"Destroyed: 6\n",
      "\"Don't: 7\n",
      "\"Hang: 8\n",
      "\"Has: 9\n"
     ]
    }
   ],
   "source": [
    "# option  1\n",
    "from itertools import islice\n",
    "w = sorted(set(preprocess))\n",
    "vocabulary1 = {key: value for key, value in enumerate(w)}\n",
    "# Print top 10 items\n",
    "print(\"\\nvocabulary:_________\")\n",
    "for key, value in islice(vocabulary1.items(), 10):\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "w = sorted(set(preprocess))\n",
    "\n",
    "\n",
    "vocabulary = {word: key for key, word in enumerate(w)}\n",
    "# Fix: This should be {word: idx} not {idx: word}\n",
    "string_to_int = {word: idx for idx, word in enumerate(w)}\n",
    "print(\"\\nstring_to_int:_____________\")\n",
    "for key, value in islice(string_to_int.items(), 10):\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb157a-2620-49e0-aa1e-239c9a49aba5",
   "metadata": {},
   "source": [
    "# Encode and Decode \n",
    "## Endcode: Simple Text -> Token text -> Token ID\n",
    "1. Simple Text (Raw Input):   \"Hello, world!\"\n",
    "2. Tokenized Text (Splitting into Tokens):\n",
    "\n",
    "               A- Whole words (\"Hello\", \"world\")\n",
    "               B- Subwords (\"unhappiness\" → \"un\", \"happiness\")\n",
    "               C- Punctuation and symbols (\",\", \"!\")\n",
    "               Tokenized Output: [\"Hello\", \",\", \"world\", \"!\"]\n",
    "3. Token IDs (Numerical Representation):\n",
    "\n",
    "                \"Hello\" → 15496\n",
    "                \",\" → 11\n",
    "                \"world\" → 995\n",
    "                \"!\" → 0\n",
    "                Final Token IDs: [15496, 11, 995, 0]\n",
    "## Decode: Token ID -> Token Text -> Simple Text\n",
    "    \n",
    "    Decode is the reverse: Token id -> Token Text -> Simple Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdb4406e-8c9c-4a35-9371-b76a749caa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1\n",
    "class Tokenization:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.string_to_int = vocabulary\n",
    "        self.int_to_string = {item: s for s,item in vocabulary.items()}\n",
    "\n",
    "    def encoding(self, text):\n",
    "        # split text around empty space, comman and period\n",
    "        preprocessing = re.split(r'([,.:?!\",()]|--|\\s)', text)\n",
    "        # Tokenize by removing all space, comma, --, etcc\n",
    "        preprocessing = [word.strip() for word in preprocessing if word.strip() ]\n",
    "        # converting into id number\n",
    "        id = [self.string_to_int[s] for s in preprocessing ]\n",
    "        return id \n",
    "\n",
    "    def decoding(self, vocabulary ):\n",
    "        # convert token id to Token text\n",
    "        tokenID_to_TokenText = \" \".join([self.int_to_string[i] for i in id])\n",
    "        # replace empty space with punctuation\n",
    "        tokenID_to_TokenText =  re.sub (r'([,.:?!\",()]|--|\\s)', r'\\1', tokenID_to_TokenText)\n",
    "        return tokenID_to_TokenText\n",
    "# option 2\n",
    "class Tokenization_1:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.string_to_int = vocabulary  # Should be {word: id}\n",
    "        self.int_to_string = {id: word for word, id in vocabulary.items()}  # Reverse mapping\n",
    "\n",
    "    def encoding_1(self, text):\n",
    "        # Extract tokens directly (no empty strings)\n",
    "        tokens = re.findall(r'[^\\s,.:?!\",()--]+|[,.:?!\",()]|--', text)\n",
    "        # Convert to IDs using dictionary lookup\n",
    "        return [self.string_to_int[token] for token in tokens]\n",
    "\n",
    "    def decode_1(self, ids):\n",
    "        # Convert list of token IDs back to their string representations\n",
    "        tokens = [self.int_to_string[id] for id in ids]\n",
    "        \n",
    "        # Reconstruct the original text by joining tokens with spaces\n",
    "        # This handles most cases where spaces were between words\n",
    "        text = ' '.join(tokens)\n",
    "        \n",
    "        # Fix punctuation spacing by removing spaces before punctuation\n",
    "        # Matches spaces before any of these punctuation marks: , . : ? ! \" , ( )\n",
    "        text = re.sub(r'\\s+([,.:?!\",()])', r'\\1', text)\n",
    "        \n",
    "        # Handle special cases like apostrophes (e.g., \"let ' s\" → \"let's\")\n",
    "        text = re.sub(r\"(\\w)\\s+'\\s+(\\w)\", r\"\\1'\\2\", text)\n",
    "        \n",
    "        # Handle double dashes (e.g., \"word -- word\" → \"word--word\")\n",
    "        text = re.sub(r'\\s+--\\s+', '--', text)\n",
    "        \n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02488619-bf3a-4606-8f19-67f56f660845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[758, 1017, 778, 706, 1347, 1471, 1491, 1351, 1522, 399, 827, 46]\n"
     ]
    }
   ],
   "source": [
    "# unseen word give us error because of short data\n",
    "Tok = Tokenization(vocabulary)\n",
    "inputText = \"height of his glory that was what the women called it.\"\n",
    "tokenID = Tok.encoding(inputText)\n",
    "print(tokenID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf71fce2-49f8-42dd-a87f-219359a9e4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[758, 1017, 778, 706, 1347, 1471, 1491, 1351, 1522, 399, 827, 46]\n"
     ]
    }
   ],
   "source": [
    "# unseen word give us error because of short data\n",
    "Tok_1 = Tokenization_1(vocabulary)\n",
    "inputText_1 = \"height of his glory that was what the women called it.\"\n",
    "tokenID_1 = Tok_1.encoding_1(inputText_1)\n",
    "print(tokenID_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e1ffd1-d217-4775-93e1-478cbae64afd",
   "metadata": {},
   "source": [
    "# How to deal with unseen token\n",
    "# Assume preprocessing is your list of tokens from dataset\n",
    "unseen_token = sorted(list(set(preprocessing)))\n",
    "\n",
    "# Add special tokens explicitly\n",
    "unseen_token.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "# Assign each token an integer ID\n",
    "vocab = {tok: num for num, tok in enumerate(unseen_token)}\n",
    "\n",
    "# Vocabulary size\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a939482d-b9d8-4a8d-8b93-f3260b168d8b",
   "metadata": {},
   "source": [
    "# Explanation of Special Tokens\n",
    "\n",
    "#### <|endoftext|>:  Marks the end of a text sequence and Used for separating documents/contexts\n",
    "\n",
    "#### <|unk|> - Represents \"unknown\" tokens: Handles words not seen during training (out-of-vocabulary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3efb56-ef58-46ee-a369-be3476603cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens to vocabulary\n",
    "special_tokens = [\"<|endoftext|>\", \"<|unk|>\"]\n",
    "base_vocab = {\"hello\": 0, \"world\": 1, \"test\": 2}  # Your normal vocabulary\n",
    "\n",
    "# Create extended vocabulary with special tokens\n",
    "extended_vocab = {}\n",
    "# Add special tokens first (usually at beginning of ID range)\n",
    "for i, token in enumerate(special_tokens):\n",
    "    extended_vocab[token] = i\n",
    "\n",
    "# Add normal vocabulary with offset\n",
    "for word, idx in base_vocab.items():\n",
    "    extended_vocab[word] = idx + len(special_tokens)\n",
    "\n",
    "print(extended_vocab)\n",
    "# {'<|endoftext|>': 0, '<|unk|>': 1, 'hello': 2, 'world': 3, 'test': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942e1b4-b666-4e7f-bb2c-2a59d3a4a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When your model encounters a word not in vocab, you map it to <|unk|>:\n",
    "def get_token_id(token, vocab):\n",
    "    return vocab.get(token, vocab[\"<|unk|>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef41a3e3-6f7d-4a4b-9365-d923c63349f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0becd7e4-0095-4f49-959d-e628d0f81809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3eee3-4daa-4ca3-8fcc-52fead0a512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"Hello, would like coffee? <|endoftext|> In the  morning in your room\"\n",
    "     \"of windburry.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500e579-f0a8-40a8-9b69-d352c164509c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
